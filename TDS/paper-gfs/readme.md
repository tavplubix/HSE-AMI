# Google File System



### Какие операции, приводящие к изменению содержимого файла поддерживает GFS? Какая операция является более частой?
Обыные записи (writes) и атомарное дописывание в конец файла (record appends). Чаще всего бывают дописывания в конец.

(2.7.1 Guarantees by GFS, "Data mutations may be writes or record appends")

(2.7.2 Implications for ApplicationsS, "Practically all our applications mutate files by appending
rather than overwriting")

### GFS хорошо подходит для больших последовательных чтений. Что из себя представляет типичный клиент, который выполняет такие чтения?

Скорее всего, это всякие мапердьюсы, которые последовательно читают файлы, как-то их преобразуют и аппендят в новые файлы, которые дальше снова кто-то читает...

Ещё, возможно, какие-нибудь LSM-Tree таблицы.
В статье нашлось только упоминание "multi-way merge results and producer-consumer queues" в (2.2 Interface) и что-то про абстрактные "research and
development" и "production data processing" в (9 CONCLUSIONS)


### Не становится ли мастер узким местом при обработке всех записей и аппендов в файловой системе? С помощью каких механизмов снижается нагрузка на мастер?
Нет, потому что мастер работает только с небольшими метаданными, работа с данными и метаданными специально резведена таким образом, чтобы данные не ходили через мастер, а ещё клиенты кешируют матаданные о том, к какому чанксерверу обратиться, а ещё мастер выдаёт чанксерверам лизы на управление чанками, после чего работа с этими чанками идёт через primary-реплику чанка, не перегружая мастер.

(2.4 Single Master)
(3.1 Leases and Mutation Order)

### Чему равен размер чанка в первоначальном дизайне GFS? А чему равен аналогичный параметр в локальных файловых системах?

Размер чанка в GFS 64MB. Для сравнения, размер блока в ext4 по умолчанию 4KB.

(2.5 Chunk Size)

### Чем мотивирована эта разница?
Большой размер чанка уменьшает размер матаданных о чанках, таким образом нужно меньше места для хранения метаданных и меньше запросов метаданных к мастеру, а ещё метаданные лучше кешируются и лучше влезают в память (в 64MB/4KB = 16384 раз меньше количество чанков, а значит размер метаданных и количество запросов матаданных). Кроме того, одна операция над большим чанком оптимальнее, чем куча операций над маленькими чанками. Т.к. файлы обычно сильно больше 64MB, оверхеда по месту на диске почти нет.

(2.5 Chunk Size, 2.6 Metadata)

### Зачем среди реплик одного чанка выделяется primary-реплика?

Primary-реплика определяет порядок мутабельных операций над чанком и сообщает об этих операциям остальным репликам чанка, чтобы они тоже применили изменения. Таким образом снижается нагрузка на мастер.

(3.1 Leases and Mutation Order)

### Как устроена процедура fail-over-а для primary-реплики чанка?

Мастер выдаёт реплике чанка лизу на управление чанком на 60 секунд, эта реплика становится primary. Мастер посылает хартбиты всем чанксерверам и, если он видит, что primary-реплика стала недоступна, то дожидается, когда истечёт время выданной лизы и отдаёт её другой реплике чанка.

(3.1 Leases and Mutation Order)

### Как упорядочиваются записи в пределах одного чанка с учетом отказов primary-реплик? Что это вам напоминает?

Записи упорядочиваются primary-репликой чанка, primary-реплики назначаются мастером. Прежде чем ответить клиенту, primary реплика дожидается подтверждения с остальных реплик, если что-то пошло не так - сообщает клиенту об ошибке, и он ретраит. Если primary-реплика стала недоступной, клиент будет ретраить с самого начала операции т.е. с обращения к мастеру, который может переназначить primary.

(3.1 Leases and Mutation Order)

### Почему записи по оффсету (даже маленькие) могут быть неатомарными?

Потому что запись может пересечь границу чанков, тогда это несколько отдельных операций записи в разные чанки, а операции с разными чанками могут управляться разными чанксерверами.

(3.1 Leases and Mutation Order)

### Может ли такое случиться при аппенде в конец файла? Почему?
Нет, потому что при аппенде оффсет выбирается не клиентом, а primary-репликой, и если запись пересекает границу чанка, то реплика просит клиента поретраить с новым чанком.

(3.3 Atomic Record Appends)

### Есть ли в первоначальном GFS единая точка отказа? Почему? Есть ли она сейчас?
Единственный мастер - точка отказа. Но мастер реплицируется, в случае падений умеет быстро переподниматься, а если всё совсем плохо (умерли диски), то это заметит какой-то мониторинг и перенесёт запустит нового мастера на одной из реплик старого мастера. Так что какое-то время кластер потупит, но быстро восстановится.

(5.1.3 Master Replication)

### Какие структуры данных образуют состояние мастера?

Неймспейсы имён файлов и чанков, маппинг из имён файлов в чанки, расположение чанков на чанксерверах (видимо, адреса реплик чанка). Чтобы сжимать пути к файлам используется что-то похожее на префиксные деревья (it stores file names compactly using prefix compression). Ещё чекпоинты состояния мастера хранятся на диске in a compact B-tree like form.

(2.6 Metadata)
(2.6.1 In-Memory Data Structures)
(2.6.3 Operation Log)

### Какие из этих структур хранятся персистентно, на диске?
Неймспейсы имён файлов и чанков, маппинг из имён файлов в чанки. Они пишутся в operation log на диск, чтобы не хранить весь лог периодически записывается checkpoint текущего состояния.

(2.6 Metadata)
(2.6.3 Operation Log)

### Какие структуры данных хранятся только в оперативной памяти? Как их восстановить в случае рестарта мастера?
Расположение чанков на чанксерверах. Опросить все чанксервера при старте мастера. Опросить новый чанксервер, когда он подключается к мастеру.

(2.6 Metadata)


### Как мастеру удалить файл, если в данный момент не доступны все реплики всех блоков?

Файлы удаляются лениво. Если файл помечен для удаления и время его жизни (3 дня) истекло, во время онового сканирования своего состояния мастер удаляет метаданные этого файла. Чанки файла становятся orphaned (не привязаны к файлам), реплики чанков узнают об этом при следующем хартбите (если они были недоступны, то при первом подключении к мастеру) и сами удалят у себя эти чанки.

(4.4 Garbage Collection)
(4.4.1 Mechanism)

### Может ли мастер обрабатывать создания файлов в пределах одной директории конкурентно? Как?

Да, мастер берёт read-lock'и на директорию (точнее, на узлы в пути т.к. директорий в привычном смысле на самом деле нет) и write-lock на имя файла. Первые защищают от конкурентных операция с директориями (которые требуют write-lock), второй - от конкурентных созданий этого же файла. Но конкурентные создания разных файлов в одной директории друг другу не мешают т.к. read-lock.

(4.1 Namespace Management and Locking)

### Что ограничивает масштабируемость GFS – данные или метаданные? Почему?

Данные хранятся на большом количестве чанксерверов, если место на них закончится, можно просто поставить больше чанксерверов, главное чтобы мастер с ними справился и чтобы ему хватило RAM для хранения состояния ("capacity of the whole system is limited by how much memory the master has"). Впрочем, метаданные в памяти мастера занимают достаточно мало места ("less than 64 bytes of metadata for each 64 MB
chunk", "less then 64 bytes per file"), так что это не сильно ограничивает масштабируемость.

(2.6.1 In-Memory Data Structures)

### От чего зависит размер метаданных GFS, из-за чего мастер может "переполниться"?
От количества чанков и файлов. Наверно, может переполниться если создать кучу маленьких файлов по одному чанку. Можно ещё попробовать давать файлам длинные рандомные имена, чтобы префиксные деревья для них плохо работали.

### Какое ограничение соблюдается при размещении реплик чанка?
Выбираются сервера в разных стойках.

(5.1.2 Chunk Replication)

### Что мастер должен сделать на уровне метаданных при создании снапшота?

Дождаться, когда отзовутся лизы на чанки файлов, записать в лог, что он делает снепшот файла (или поддерева), скопировать в памяти метаданные файла (или поддерева). Ещё ему нужно взять read-lock'и для всех элементов в пути, с которого он делает снепшот, и write-lock'и для всех элементов пути, по которому он записывает снепшот.

(3.4 Snapshot)
(4.1 Namespace Management and Locking)

### Зачем при создании снапшота файла мастер отзывает все лизы у primary-реплик чанков этого файла?

Мастер делает снепшоты через copy-on-write. Когда клиент попытается записать в файл, ему придётся пойти к мастеру (т.к. лиза отозвана и нет primary-реплики чанка), мастер поймёт что кто-то хочет записать и нужно сначала скопировать файл, а потом разрешать писать в него.

(3.4 Snapshot)

### Зачем мастеру лог? Может ли мастер хранить его в GFS?
Чтобы восстанавливаться после падений, очевидно. Нет, потому что после падения нам нужно восстановить из лога метаинформацию, прежде чем обращаться к чанксерверам (иначе мы не знаем из каких чанков состоит файл и к кому обращаться).

### Что такое осиротевший чанк?

Чанк, который не принадлежит файлу.

(4.4.1 Mechanism)


### Что происходит, когда умирает чанк-сервер? Как мастер узнает о том, что какой-то чанк нужно дореплицировать?

Чанксервер перестаёт присылать мастеру хартбиты с метаинформацией о своих чанках, мастер понимает, что чанксервер недоступен. Аналогично мастер узнает о необходимости дореплицировать чанк, если чанксервер пришлёт ему хартбит с неправильным хэшом чанка (чанк побился). Также мастер поддерживает версию чанка (увеличавается с каждой выданной лизой), чтобы понимать, что какая-то реплика чанка отстала и не применила последние изменения.

(4.5 Stale Replica Detection)
(5.2 Data Integrity)

### Как можно снизить оверхед от репликации, не снижая при этом отказоустойчивость чанка? Как этому препятствует API системы?

Можно использовать parity or erasure codes и прочие коды с коррекцией ошибок, но тогда будет недостаточно читать данные с одного сервера, а чтение - очень частая операция.

(5.1.2 Chunk Replication)

### Как BigTable использует GFS?
Он хранит в GFS свои данные.

### Как в HDFS называются мастер и чанк-сервер соответственно?
Master - namenode, chunkserver - datanode.
